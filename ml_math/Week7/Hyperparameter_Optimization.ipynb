{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter Optimization\n",
    "\n",
    "### GaussianProcessRegressor vs GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import sklearn.gaussian_process.kernels as kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Full path of the file\n",
    "'''\n",
    "FILEPATH_TRAIN = 'train.csv'\n",
    "FILEPATH_TEST = 'test.csv'\n",
    "'''\n",
    "The number of classes\n",
    "'''\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_test_data():\n",
    "    df = pd.read_csv(FILEPATH_TRAIN)\n",
    "    dfTest = pd.read_csv(FILEPATH_TEST)\n",
    "    return df,dfTest\n",
    "\n",
    "def extract_train_test_matrixes(df,dfTest):\n",
    "    X = df.iloc[:,0:len(df.columns)-(NUM_CLASSES+1)].as_matrix()\n",
    "    y = df.iloc[:,-1].as_matrix()\n",
    "\n",
    "    Xval = dfTest.iloc[:,0:len(df.columns)-(NUM_CLASSES+1)].as_matrix()\n",
    "    yval = dfTest.iloc[:,-1].as_matrix()\n",
    "    \n",
    "    return X,y,Xval,yval\n",
    "    \n",
    "\n",
    "def print_evaluation_summary(labels,predictions):\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    correct = np.sum(np.equal(labels,predictions))\n",
    "    total = len(labels)\n",
    "    acc = correct/total\n",
    "    \n",
    "    print('Dataset information:')\n",
    "    print('Num observations: %d' % int(labels.shape[0]))\n",
    "    print('Num classes: %d\\n' % NUM_CLASSES)\n",
    "    \n",
    "    print('Evaluation:')\n",
    "    print('Total hits: %d/%d - pct: %.3f\\n' % (correct,total,acc))\n",
    "    \n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(str(metrics.confusion_matrix(labels,predictions))+'\\n')\n",
    "    \n",
    "    print('Classification Report: '+'\\n')\n",
    "    print(metrics.classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,dfTest = read_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,Xval,yval = extract_train_test_matrixes(df,dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11013, 18), (11013,), (380, 18), (380,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape,Xval.shape,yval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
       "             kernel=RBF(length_scale=1), n_restarts_optimizer=0,\n",
       "             normalize_y=False, optimizer='fmin_l_bfgs_b', random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = kernels.RBF(length_scale_bounds=\"fixed\")\n",
    "gp = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "gp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predGP = np.clip(np.around(a=gp.predict(Xval),decimals=0),a_min=0,a_max=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "A grid search (done on another Notebook) gives us the next result as best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 30\n",
    "gamma = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC(C=C, gamma=gamma,kernel='rbf')\n",
    "fitted_svc = svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predGridSearch = svc.predict(Xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Head2Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "Num observations: 380\n",
      "Num classes: 3\n",
      "\n",
      "Evaluation:\n",
      "Total hits: 203/380 - pct: 0.534\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[177   0   4]\n",
      " [ 83   0   6]\n",
      " [ 84   0  26]]\n",
      "\n",
      "Classification Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.98      0.67       181\n",
      "          1       0.00      0.00      0.00        89\n",
      "          2       0.72      0.24      0.36       110\n",
      "\n",
      "avg / total       0.45      0.53      0.42       380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_summary(yval,predGridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "Num observations: 380\n",
      "Num classes: 3\n",
      "\n",
      "Evaluation:\n",
      "Total hits: 133/380 - pct: 0.350\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[79 77 25]\n",
      " [43 36 10]\n",
      " [30 62 18]]\n",
      "\n",
      "Classification Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.44      0.47       181\n",
      "          1       0.21      0.40      0.27        89\n",
      "          2       0.34      0.16      0.22       110\n",
      "\n",
      "avg / total       0.39      0.35      0.35       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_summary(yval,predGP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
